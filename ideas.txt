TODO

create the job scripts
create slideshow
create readme/documentation for how to use this thing
create model parallelism



main ice article: https://gatech.service-now.com/technology?id=kb_article_view&sysparm_article=KB0042102


https://gatech.service-now.com/home?id=kb_article_view&sysparm_article=KB0042095
show all the different resources on ice, could go into like how to pull each one

monitoring jobs, and interactive and sbatch this is a really important article
https://gatech.service-now.com/home?id=kb_article_view&sysparm_article=KB0042096


personal notes/random crap

brief review of the intro to pace
moving files around, on demmand, interactive jobs, sbatch jobs
jupyter notebooks on cluster???

pytorch cpu/gpu
pulling weights for inference/moving files around easily
running pytorch on ice and the cpu/gpu
the weights file
pytorch ddp
model parallelism


supercomputer notes
login ssh zbuchholz3@login-ice.pace.gatech.edu make sure vpn on or on eduroam

copy stuff over secure copy command scp -r: scp -r Workshop1-IntroML/ zbuchholz3@login-ice.pace.gatech.edu:~ the tilde you can change to specify exactly where you want it to go
nope scp is shit use async
-avz a (archive mode preserve like timestamps, permissions etc) -v verbose -z compress to send over faster
rsync -avz --progress --exclude-from='Workshop1-IntroML/.rsyncignore' Workshop1-IntroML/ zbuchholz3@login-ice.pace.gatech.edu:~/Workshop1-IntroML/

get weights file off
rsync -avz --progress zbuchholz3@login-ice.pace.gatech.edu:~/Workshop1-IntroML/src/cifar_model.pth ./Workshop1-IntroML/src/

module avail - see all available modules
module load anaconda3

conda create -n ml_workshop
conda activate ml_workshop
pip install -r requirements.txt
conda deactivate

gpu type is optional you will just be give the first gpu
--gres=gpu:<gpu type>:<number of gpus per node>

salloc -greswhatever --ntasks-per-node=1
salloc --gres=gpu:1 --ntasks-per-node=1 --cpus-per-task=4 --mem=32G --time=3:00:00
salloc --gres=gpu:4 --ntasks-per-node=4 --cpus-per-task=4 --mem=32G --time=3:00:00

see info about the nodes and how many cpus/gpus they have
sinfo -o "%20N %10c %10m %25f %10G"

nvidia-smi to see info about gpu

local rank gpu id on current node

python -m torch.distributed.run --nproc_per_node=4 train_ddp.py


slide outline

intro slide
table of contents?

objective slide
- use pace to train nn
- use the weights locally
- speedup network using data and model parallelism

brief problem overview
- using the cifar10 dataset
- show the example images, classification problem, 10 classes

network overview
- show the network we will use to solve this
- diagram of the cnn layers and fc layers with their dimensions
- techniques used to improve - dropout, batch norm, data augmentation etc.

brief pytorch overview??
- maybe optional
- show and explain the model code and training code
- explain how the weights can be saved to disk and used again, def mention this somewhere if we don't have this slide

pace
- getting the network code onto pace,
- 3 ways, github, send file through terminal, on demmand

running the code on pace
- interactive session/batch jobs quick review
- this could be a good time to go more in depth than the original workshop on like checking queues and monitoring jobs etc https://gatech.service-now.com/home?id=kb_article_view&sysparm_article=KB0042096
- could also go over how to view all of the different resources like gpus/cpus etc on ice and how to request different ones https://gatech.service-now.com/home?id=kb_article_view&sysparm_article=KB0042095

inference
- pull the weights off
- inference on local machine or can inference on supercomputer
- at least mention that you can inference locally cuz thats like a big part of training on a huge system

Data parallelism
- overview of what it is, copy model to several gpus
- distributed samplers
- the setting up of ddp and the rank etc
- converting model to ddp and training with ddp
- running a ddp model

Model parallelism
coming soon lol
